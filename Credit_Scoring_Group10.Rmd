---
title: "Group 10 - Credit Scoring"
author: "Timmy Tschannen, Sarah Lea Sch√ºrch, Keijo Alexander Nierula, Roman Krass"
date: "2023-11-07"
output: 
  html_document:
    toc: true
    toc_depth: 2
    toc_float: true
    number_sections: true
---

```{r setup, include=FALSE, echo=FALSE}
# Getting started by changing the default output of echo to TRUE for the current document 
knitr::opts_chunk$set(echo = TRUE, cache=TRUE)

# Create a list of packages to install and load into the work space
libraries = c("readr", "tidyverse", "dlookr", "ggplot2", "Hmisc", "reshape2", "plotly", "caret", "ROCR", "kableExtra","Boruta")
 
# Install packages from the predefined libraries list
lapply(libraries, function(x) if (!(x %in% installed.packages())) {
  install.packages(x)
})

# Load libraries
lapply(libraries, library, quietly = TRUE, character.only = TRUE)

# Remove current environment 
rm(list=ls())
```

# Project Setup {.unnumbered}

```{r }
# Set seed
set.seed(7)

# Importing data
loanData <- read_csv("loan_sample_10.csv")
data <- loanData

# Checking structure of the data
head(data)
tail(data)
```

The `head()` and `tail()` functions show, that there is no unusual observation and the data set looks complete and has no emtpy rows at the end.

```{r graph_setup, results="hide", message=FALSE}
# Define colors to create a general style 
custom_fill <- "#105F77"
custom_alpha <- 0.8
custom_color <- "#161616"
custom_light <- "#f4a460"
custom_dark <- "#105F77"
```

# Checking data for NA values {.unnumbered}

```{r}
# Check for missing values in each column of the data set and store the result in the "has_NAs" data frame
has_NAs = as.data.frame(apply(data, 2, function(x) any(is.na(x))))

# Map matching column names for a more readable output
colnames(has_NAs) = c("has_NA")

# Print the has_NAs data frame to check if there are any NA values in our data set
has_NAs
```

We also found out, that none of the variables has any `NA` values.

# Description of variables {.unnumbered}

This list describes the variables of the dataset loan_data:

annual_inc: The self-reported annual income provided by the borrower during registration\
application_type: Indicates whether the loan is an individual or joint application\
dti: Borrower's total monthly debt payments divided by monthly income\
grade: Assigned loan grade by the financial service provider\
home_ownership: The home ownership status\
int_rate: Interest Rate on the loan\
loan_amnt: The listed amount of the loan applied for by the borrower\
open_acc: Number of open trades in last 6 months\
purpose: A category provided by the borrower for the loan request\
revol_bal: Total credit revolving balance\
revol_util: Revolving line utilization rate\
tot_cur_bal: Total current balance of all accounts\
total_acc: The total number of credit lines currently in the borrower's credit file\
total_rec_int: Interest received to date\
total_rev_hi_lim: Total revolving high credit/credit limit\
verification_status: Indicates if the co-borrowers' joint income was verified

------------------------------------------------------------------------

<!-- ----------- Exercise 1 ----------- -->

# Exercise 1

## Describe the data. Specifically:

<!-- ----------- Exercise 1.1 ----------- -->

### Check and report the structure of the data set.

```{r}
# First, we check the dimension of the dataset
dim(data)
```

With the `dim` function we see, that there are 40'000 observations within 17 variables.

```{r}
# Next, we display the structure of the dataset, showing the type of each variable (numeric, categorical, etc.).
str(data)
```

The `str` function shows us all the different variables and what type they're from.

### How many numeric and how many categorical variables are included in the data? What categorical variable has the most levels in it?

```{r}
num_vars <- sum(sapply(data, is.numeric))
cat_vars <- sum(sapply(data, is.character))

# Print counts
cat("Number of numeric variables:", num_vars, "\n")
cat("Number of categorical variables:", cat_vars, "\n")
```

Reporting the structure of the data set shows us, that we have `r as.character(cat_vars)` character columns and `r as.character(num_vars)` number columns.

### Summarize the variables. Discuss the summary statistics obtained.

```{r}
summary(data)
```

While checking the summary, we can observe, that there might be some values with insane outliers. For example `annual_inc`, `revol_bal` or `tot_cur_bal`. These variables all have a small minimum compared to the median and the max value. This could mean that either the data is very wide spread or there are a few outliers that have an impact on the median and the mean.

### Check the levels of the target variable by choosing the appropriate visualization. Is the target variable balanced?

```{r}
ggplot(data, aes(x = factor(Status))) +
  geom_bar(fill = custom_fill, color = custom_color) +
  labs(x = "Loan Status", y = "Count", title = "Distribution of Loan Status") +
  theme_minimal(base_size = 22)
```

Checking the levels of the `loan status` variable, we can see, that the distribution is not balanced. There are much more entries with status = 0 than status = 1.

### Check the distribution of the numeric variables in the data set (include different visual representations).

```{r}
# This code generates a histogram for each numeric variable

# Function to plot histogram for each numeric variable
plot_histograms <- function(data) {
  numeric_vars <- sapply(data, is.numeric)
  data_numeric <- data[, numeric_vars]

  # Loop through each numeric variable
  for(var in names(data_numeric)) {
    print(
      ggplot(data, aes_string(x = var)) +
        geom_histogram(bins = 30, fill = custom_fill, color = custom_color) +
        labs(x = var, y = "Frequency", title = paste("Distribution of", var)) +
        theme_minimal(base_size = 22)
    )
  }
}

# Apply the function to the loan dataset
plot_histograms(data)

```

------------------------------------------------------------------------

<!-- ----------- Exercise 1.2 ----------- -->

## Analysing Outliers

```{r}
# Function to plot box plots for each numeric variable
# The function plot_boxplots identifies all numeric variables in the dataset and creates box plots for each.
plot_boxplots <- function(data) {
  numeric_vars <- sapply(data, is.numeric)
  data_numeric <- data[, numeric_vars]

  # Loop through each numeric variable
  for(var in names(data_numeric)) {
    print(
      ggplot(data, aes_string(y = var)) +
        geom_boxplot(fill = custom_fill, color = custom_color) +
        labs(y = var, title = paste("Boxplot of", var)) +
        theme_minimal(base_size = 22)
    )
  }
}

# Apply the function to the loan dataset
plot_boxplots(data)

```

------------------------------------------------------------------------

<!-- ----------- Exercise 1.3 ----------- -->

## Visualization to display the distribution of the numeric features

To visualize the distribution of numerical features, box plots provide a clear summary of the data distribution and highlight medians, quartiles and outliers. When split by a categorical variable such as loan status, they allow for direct comparison between groups. This is particularly useful for identifying which numerical characteristics might behave differently in the two categories of the target characteristic and provides insight into which variables might be significant predictors.

```{r distribution-numerical, echo=TRUE}
# Visualizing Numeric Features by Target Feature
# This code generates box plots for each numeric variable, separated by the target feature 'Status'.
# Function to create box plots for each numeric variable, separated by loan status
plot_numeric_by_target <- function(data, target) {
  numeric_vars <- sapply(data, is.numeric)
  data_numeric <- data[, numeric_vars]

  for (var in names(data_numeric)) {
    if (var != "Status") {
      # Corrected usage of print() to render the plots
      print(
        ggplot(data, aes(x = as.factor(target), y = !!sym(var), fill = as.factor(target))) +
        geom_boxplot(fill = custom_fill, color = custom_color) +
        labs(title = paste("Distribution of", var, "by Status"), x = "Status", y = var) +
        theme_minimal(base_size = 22)
      )
    }
  }
}

# Apply the function to the loan dataset
plot_numeric_by_target(data, data$Status)
```

**Discussion of the visualization**\
After running the code below, we will get box plots for each numeric variable, broken down by loan status. These plots show the median (middle line in the box) of each group, the dispersion (size of the box), which shows the variability and the ossible outliers (points outside the whiskers).

Based on the generated boxplots for a subset of numeric variables in the loan_data dataset, in this section is our discussion of the findings presented:\
**Loan Amount (loan_amnt)**\
The distribution of loan amounts seems relatively similar for both default and non-default groups. However, there's a slight indication that higher loan amounts might be more prevalent in the default group. This suggests that as the loan amount increases, there might be a slightly higher risk of default.

**Interest Rate (int_rate)**\
The interest rate shows a notable difference between the two groups. Loans that end up in default tend to have higher interest rates. This is evident from the higher median and the spread of the box for the default group. Higher interest rates could be an indicator of increased risk associated with the loan, leading to a higher likelihood of default.

**Annual Income (annual_inc)**\
The annual income of borrowers does not show a significant difference in the medians of the two groups, although the non-default group seems to have a slightly higher income range. This might suggest that while income is a factor, it's not a strong differentiator between default and non-default cases on its own.

**Debt-to-Income Ratio (dti)**\
The debt-to-income ratio appears slightly higher for the default group. This suggests that a higher DTI ratio might be associated with a greater risk of default. The higher spread in the default group indicates a wider variability in DTI ratios among those who default.

**Revolving Balance (revol_bal)** The revolving balance shows a relatively similar distribution across both groups, with a slightly higher median in the non-default group. This indicates that revolving balance alone might not be a strong predictor of default.

From this analysis, it seems that the interest rate and debt-to-income ratio are particularly relevant variables in predicting the likelihood of loan default. Higher interest rates and higher DTI ratios are more common in loans that default, suggesting these factors are important in assessing credit risk. While loan amount, annual income, and revolving balance do show some differences, their impact appears to be less pronounced compared to interest rates and DTI ratios.

------------------------------------------------------------------------

<!-- ----------- Exercise 1.4 ----------- -->

## Visualization to display the distribution of the categorial features

The code in this section creates bar plots for each categorical variable, showing their association with the loan status (default vs. non-default).

```{r distribution-categorial}

# Function to plot bar plots for each categorical variable
plot_categorical_associations <- function(data) {
  categorical_vars <- sapply(data, is.character) # or is.character, depending on how data is loaded
  data_categorical <- data[, categorical_vars]

  for(var in names(data_categorical)) {
    # Create the plot and assign it to a variable
    plot <- ggplot(data, aes(x = !!sym(var), fill = factor(data$Status))) +
      geom_bar(position = "stack") +
      labs(title = paste("Association between", var, "and Status"), x = var, y = "Count") +
      theme_minimal(base_size = 22) +
      scale_fill_brewer(palette = "Set1") +
      guides(fill = guide_legend(title = "Status"))+
    theme(axis.text.x = element_text(angle = 45, hjust = 1))
    
    # Print the plot
    print(plot)
  }
}

# Apply the function to the loan dataset
plot_categorical_associations(data)

```

From these plots, we can observe the proportion of default and non-default cases within each category of the categorical variables. The significant differences in the proportion of default vs. non-default cases across the categories of a variable suggest a potential association. For example, if a certain loan purpose or grade shows a noticeably higher proportion of defaults, it may indicate that these variables are relevant in predicting loan defaults.

The following points describe our findings:\
**Loan Grade (grade):**\
There is a noticeable trend in loan defaults across different loan grades. Lower grades (typically representing higher risk) show a higher proportion of defaults. This suggests that the loan grade is a significant factor in predicting loan defaults.

**Home Ownership (home_ownership)**\
The distribution of loan status varies across different home ownership categories. However, the variation is not as pronounced as with loan grades. It appears that borrowers with mortgages have a slightly higher count of non-defaults compared to other categories.

**Verification Status (verification_status)**\
The verification status of the borrower also shows an interesting pattern. Loans with verified statuses tend to have a lower proportion of defaults compared to those that are not verified or are source verified. This implies that verification status might play a role in predicting loan defaults.

**Purpose of the Loan (purpose)**\
The purpose of the loan demonstrates varying patterns of defaults. Certain purposes, like debt consolidation, show a higher number of defaults. This variation suggests that the reason for taking out a loan could be an indicator of default risk.

**Application Type (application_type)**\
The application type, whether individual or joint, shows some differences in default rates. Individual applications seem to have a slightly higher proportion of defaults compared to joint applications, indicating that the type of application might be a factor in default risk.

From these observations, it's evident that certain categorical variables such as loan grade and loan purpose show a more pronounced association with loan status, indicating their potential relevance in predicting loan defaults. Variables like home ownership and application type also show some association but to a lesser degree. The verification status, interestingly, has a noticeable impact, suggesting that the level of scrutiny in the loan approval process can affect the likelihood of a loan defaulting. These insights are valuable for understanding the risk factors associated with loan defaults and can guide further analysis and modeling efforts.

------------------------------------------------------------------------

<!-- ----------- Exercise 1.5 ----------- -->

## Visualization of the correlations

The heatmap of the correlation matrix for the numeric features in the loan_data dataset reveals the following insights:

```{r correlations, echo=TRUE}
# The code first selects only numeric variables from our dataset. It then calculates the correlation matrix using Pearson correlation. The melt function from the reshape2 package is used to transform the correlation matrix into a long format suitable for ggplot. Finally, a heatmap is plotted using ggplot2 with appropriate color scales to represent the strength and direction of correlations.

# Selecting only the numeric variables from the dataset
numeric_vars <- data[sapply(data, is.numeric)]

# Calculate the correlation matrix
correlation_matrix <- cor(numeric_vars, use = "complete.obs")

# Melt the correlation matrix for visualization
melted_correlation_matrix <- melt(correlation_matrix)

# Plotting the correlation matrix
ggplot(melted_correlation_matrix, aes(Var1, Var2, fill = value)) +
  geom_tile() +
  scale_fill_gradient2(low = custom_light, high = custom_dark, mid = "white", 
                       midpoint = 0, limit = c(-1,1), space = "Lab", 
                       name="Pearson\nCorrelation") +
  theme_minimal(base_size = 22) +
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1)) +
  labs(x = '', y = '', title = 'Correlation Matrix of Numeric Features')
```

------------------------------------------------------------------------

<!-- ----------- Exercise 1.6 ----------- -->

## Visualization of the correlations

The scatter plot showing the relationship between the loan amount requested and the annual income of the borrowers reveals the following insights:

```{r scatterplot, echo=TRUE}
# We plot an interactive scatter plot showing the association between the loan amount requested and the annual income of the borrower
# Creating an interactive scatter plot
plot <- plot_ly(data, x = ~annual_inc, y = ~loan_amnt, type = "scatter", mode = "markers",
                marker = list(size = 10, opacity = 0.5, color = custom_fill),
                hoverinfo = "text",
                text = ~paste("Annual Income:", annual_inc, "<br>Loan Amount:", loan_amnt))
                
# Adding layout details
plot <- plot %>% layout(title = "Scatter Plot of Loan Amount vs Annual Income",
                        xaxis = list(title = "Annual Income"),
                        yaxis = list(title = "Loan Amount"))

# Display the plot
plot
```

**Association Between Loan Amount and Annual Income**\
Positive Correlation: There appears to be a positive correlation between the loan amount and the annual income. As the annual income increases, the loan amounts also tend to increase. This suggests that borrowers with higher incomes are likely to request larger loans.

**Variability in Loan Amounts**\
Despite the positive trend, there is considerable variability in the loan amounts across different income levels. This indicates that factors other than income also play a significant role in determining the loan amount.

**Income Range**\
The plot shows a wide range of incomes among borrowers. Those with lower incomes tend to request smaller loans, while those with higher incomes have a broader range of loan amounts.

**Outliers and Spread**\
There are some outliers, particularly at higher income levels, where borrowers with very high incomes request relatively small loan amounts. Additionally, the spread of loan amounts widens with increasing income, suggesting more variability in the loan amounts requested by higher-income borrowers.

**Conclusion and Decision on Variables**\
High Correlation: Given the positive correlation, both annual_inc (annual income) and loan_amnt (loan amount) are important variables for predicting loan behavior and should be considered in the predictive model.

Keeping Both Variables: Although they are correlated, both variables likely contribute unique information. Annual income is a key factor in a borrower's ability to repay, while the loan amount is directly related to the loan's terms and risk. Therefore, it would be beneficial to keep both variables in the model.

In summary, the association between loan amount and annual income is significant and positive, indicating the importance of these variables in the predictive model. Their relationship suggests that while income is a factor in determining loan amounts, other variables also influence the final loan amount requested.

------------------------------------------------------------------------

<!-- ----------- Exercise 1.7 ----------- -->

## New balanced data set

In this step we creating a balanced dataset where the two levels of the target variable ('Status') are equally represented, and then we visualize this with a bar plot.

```{r balanced, echo=TRUE}
# In this last step we build a balanced dataset

# Check the balance of the original dataset
table(data$Status)

# Create a balanced dataset
# First, separate the dataset into two based on the target variable
data_default <- data %>% filter(Status == 1)
data_no_default <- data %>% filter(Status == 0)

# Then, randomly sample from the larger group to match the size of the smaller group
set.seed(7) # for reproducibility
data_no_default_balanced <- data_no_default %>% sample_n(nrow(data_default))

# Combine the two balanced datasets
balanced_data <- rbind(data_default, data_no_default_balanced)

# Create a bar plot of the newly created target variable
ggplot(balanced_data, aes(x = factor(Status))) +
  geom_bar(fill = custom_fill, color = custom_color) +
  labs(title = "Bar Plot of Balanced Target Variable", x = "Loan Status", y = "Count") +
  theme_minimal(base_size = 22)

# Check the balance of the new dataset
table(balanced_data$Status)
```

Balancing the Dataset is important for the following different points:

**Avoid Bias in Models**\
When a dataset is imbalanced, especially in a binary classification problem, models can become biased towards the majority class. This can lead to poor generalization performance on the minority class.

**Better Performance Metrics**\
Balanced datasets provide a more realistic evaluation of model performance metrics, especially those sensitive to class imbalance like accuracy, precision, and recall.

**Reflect Real-World Scenarios**\
In some cases, balancing a dataset can help models learn patterns that are not overrepresented by the majority class, thus better reflecting real-world scenarios where the target classes are more evenly distributed.

<!-- ----------- Exercise 2 ----------- -->

# Exercise 2

<!-- ----------- Exercise 2.1 ----------- -->

## Create a training and test data set

Divide the sample into training and testing set using 70% for training the algorithm.

```{r, trainingData}
# Set the seed again because it sometime gets lost
set.seed(7)

# Split the data into training and test sets. 70% of the data will be used for training and 30% for testing.
balanced_data$Status <- as.factor(balanced_data$Status)
div <- createDataPartition(y = balanced_data$Status, p = 0.7, list = F)

# Training Sample
data.train <- balanced_data[div,]

# Test Sample
data.test <- balanced_data[-div,]

```

<!-- ----------- Exercise 2.2 ----------- -->

## Create a logistic regression model

Train the classifier and report the coefficients obtained and interpret the results.

```{r, training}

# Set character variables which are not a factor as factor so the glm function can work with them
mutate(data.train, Status = as.factor(Status),
        application_type = as.factor(application_type),
        grade = as.factor(grade),
        home_ownership = as.factor(home_ownership),
        purpose = as.factor(purpose),
        verification_status = as.factor(verification_status))

model <- glm(Status ~ ., data = data.train, family = binomial())

summary(model)

```

In the Summary it is visible, that not all of the variables have a significant impact on the model. Because of that we will only extract the coefficients of the significant variables in the next step and will analyze them.

```{r}
# Extract the significant variables from the model
significant.variables <- summary(model)$coeff[-1,4] < 0.05
names.significant.variables <- names(significant.variables)[significant.variables == TRUE]


# Create a table with the significant variables and their coefficients
data.frame(Variable = names(coef(model)[names.significant.variables]), Coefficient = coef(model)[names.significant.variables]) %>%
  rownames_to_column(var = "rowname") %>%
  select(-rowname) %>%
  arrange(desc(Coefficient)) %>%
  kable("html") %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"), full_width = F, position = "left")


```

As visible in the table above all variables with a positive coefficient increase the chance of a default of the loan. The bigger the number is the higher is the chance for the default.\
In contrast all values with a negative value decrease the chance for a default. The smaller the number, the less likely it is that the loan defaullts.

<!-- ----------- Exercise 2.3 ----------- -->

Plot the ROC and the Precision/Recall Curve and interpret the results.

```{r, ROC}

# Test the perfromance of the model with the test data
data.test$fit_score <- predict(model,type='response',data.test)
model_preddiction <- prediction(data.test$fit_score, data.test$Status)
model_roc <- performance(model_preddiction, "tpr", "fpr")

# Plot the ROC Curve
plot(model_roc, lwd=1, colorize = TRUE, main = "Model to predict Status - ROC Curve", 
  cex.main=2, 
  cex.axis=1.7, 
  cex.lab=2)
lines(x=c(0, 1), y=c(0, 1), col="black", lwd=1, lty=3)

```

The ROC curve should be ideally as close as possible to the top left corner. This means that the model has a high true positive rate and a low false positive rate. In our ROC curve we can see, that the model is not very good. The curve is very close to the diagonal line. This means that the model is not very good in predicting the status of the loan.

<!-- ----------- Exercise 2.4 ----------- -->

## Create a confusion matrix

Produce the confusion matrix and interpret the results.

```{r, confusionMatrix}
# Create the confusion matrix
confusionMatrix <- confusionMatrix(as.factor(round(data.test$fit_score)), data.test$Status)
confusionMatrix
```

In the confusion matrix we can see that we have 974 true positives, 543 false positives, 567 false negatives and 998 true negatives. The values which is bad for a bank is the FP value, because in this case they loose money. The FN value is not as bad, because the bank does not loose money, but they also don't earn money. In our case these two values are nearly the same. Ideally we would try to decrease the FP values.

<!-- ----------- Exercise 2.5 ----------- -->

## AUC and Accuracy

Report the AUC values and the overall accuracy and interpret the results.

```{r, AUC}

# Get the AUC values
fit_auc <- performance(model_preddiction, measure = "auc")
print(paste("AUC: ",fit_auc@y.values[[1]]*100))
print(paste("Accuracy: ", confusionMatrix$overall['Accuracy']*100))

```

The AUC percentage of our model is nearly 69%. This is not that bad but also not very good. This means that our model can seperate the class in 69% of the cases. The accuracy of our model is nearly 64%. This is not very good. This means that our model can predict the status of the loan in 64% of the cases. This is not far away from a random 50% guess.


<!-- ----------- Exercise 3 ----------- -->

# Exercise 3


## Pre-processing steps

In this exercise we review the data pre-processing steps that we carried out before training our model.

* Missing values:
  * The initial dataset had no missing values, so in this case there is no room for optimization.
* Outliers:
  * The analysis showed that there were some outliers present, especially in the annual income variable.
  * The outliers can be removed by applying a function and substituting the outliers by the 5th or 95th quantile value. This could have a chance to improve the predictive performance of the model.
* Categorial Variables
  * We converted character variables to factors in Exercise 2.2, to assist the model in the training process.
  

## Other ways to improve the model

* Feature Selection
  * We could try to select the most important features to improve the predictive performance of the model, respectively ignore the insignificant variables in the model.
* Extended Data Collection
  * The bank could think about collecting more data to improve the model. For example the bank could collect more data about the customers, like their age, their job, their credit score, their credit history, their debt-to-income ratio, the interest rate etc. While these attributes could improve the prediction a lot, they should be handled with care and be e.g. be anonymised for data protection reasons.


## Suggested Improvements

We try to improve the model by applying the following steps.

After balancing the dataset we substitute the outliers in the annual income variable:
This example shows with the scatterplot, that the outliers for the annual income variable are substituted by the 5th and 95th quantile.
```{r}

substitute_outliers <- function(x) {
  qnt <- quantile(x, probs=c(.05, .95))
  x[x < qnt[1]] <- qnt[1]
  x[x > qnt[2]] <- qnt[2]
  return(x)
}

balanced_data$annual_inc <- substitute_outliers(balanced_data$annual_inc)

# We now display the scatterplot of annual income and loan amount again, here we can see that the outliers are removed:
plot <- plot_ly(balanced_data, x = ~annual_inc, y = ~loan_amnt, type = "scatter", mode = "markers",
                marker = list(size = 10, opacity = 0.5, color = custom_fill),
                hoverinfo = "text",
                text = ~paste("Annual Income:", annual_inc, "<br>Loan Amount:", loan_amnt))
                
# Adding layout details
plot <- plot %>% layout(title = "Scatter Plot of Loan Amount vs Annual Income",
                        xaxis = list(title = "Annual Income"),
                        yaxis = list(title = "Loan Amount"))

# Display the plot
plot

```


In the next step, we apply the substitute_outliers function to all the numeric variables in the dataset.

```{r}
balanced_numeric <- map_df(balanced_data[,-c(3,4,5,6,7,14,17)], substitute_outliers)
cols <- balanced_data[,c(3,4,5,6,7,14,17)]
balanced_new <- cbind(balanced_numeric, cols)
```


## Significance of variables (Boruta)
We apply the Boruta algorithm to find out, which variables are insignificant to the Status indicator.
```{r, echo=FALSE}
balanced_new$Status <- as.factor(balanced_new$Status)

boruta_output <- Boruta(Status~., data = balanced_new, doTrace=2)

boruta_signif <- getSelectedAttributes(boruta_output, withTentative = TRUE)
print(boruta_signif)


plot(boruta_output, cex.axis=.7, las=2, xlab="", main="Variable Importance")
```

The plot shows that the application_type variable is not important to the model, so we remove it:
```{r}
balanced_new <- balanced_new[,-c(16)]
```


## Apply the improvements to the model
In this step we rebuilt the model with the new dataset, as we did in exercise 2.
```{r}
# Set the seed again because it sometime gets lost
set.seed(7)

# Split the data into training and test sets. 70% of the data will be used for training and 30% for testing.
balanced_new$Status <- as.factor(balanced_new$Status)
div <- createDataPartition(y = balanced_new$Status, p = 0.7, list = F)

# Training Sample
data.train <- balanced_new[div,]

# Test Sample
data.test <- balanced_new[-div,]

model <- glm(Status ~ ., data = data.train, family = binomial())

summary(model)

```


## Test the new model
```{r}
# Test the perfromance of the model with the test data
data.test$fit_score <- predict(model,type='response',data.test)
model_preddiction <- prediction(data.test$fit_score, data.test$Status)
model_roc <- performance(model_preddiction, "tpr", "fpr")

# Plot the ROC Curve
plot(model_roc, lwd=1, colorize = TRUE, main = "Model to predict Status - ROC Curve", 
  cex.main=2, 
  cex.axis=1.7, 
  cex.lab=2)
lines(x=c(0, 1), y=c(0, 1), col="black", lwd=1, lty=3)


confusionMatrix <- confusionMatrix(as.factor(round(data.test$fit_score)), data.test$Status)
confusionMatrix

fit_auc <- performance(model_preddiction, measure = "auc")
print(paste("AUC: ",fit_auc@y.values[[1]]*100))
```


## Results of the improvement
The accuracy of the model is now at 64.21%, which is not a noticeable improvement to the initial value of 63,98%.
The AUC value is now 68.72%, which is even lower than initially (68.9%).

This means that the improvements that we tried, like removing the outliers and the application_type variable, did not improve the model.
We think that the reason for this could be that the dataset is maybe not big enough to improve the model by removing outliers and certain variables.

But with a more extensive dataset, and maybe with some additional information about the customers, the model could be improved more.
One could also think about feature engineering e.g. combining certain values to new variables, like for example the ratio between loan amount and annual income.


<!-- ----------- Exercise 3 ----------- -->

# Exercise 4







